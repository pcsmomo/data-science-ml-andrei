{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXk21dwtySRX",
        "outputId": "47c8dcae-00a2-43a8-cde1-cca9ef99c9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.9.2\n",
            "TF Hub version: 0.12.0\n",
            "GPU available (YESSSS!!!!!)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary tools\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub \n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"TF Hub version:\", hub.__version__)\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"GPU\", \"available (YESSSS!!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVBL0DjaLOED"
      },
      "source": [
        "## Getting our data ready (turning into Tensors)\n",
        "\n",
        "With all machine learning models, our data has to be in numerical format. So that's what we'll be doing first. Turning our images into Tensors (numerical representations).\n",
        "\n",
        "Let's start by accessing our data and checking out the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKeiQqQOdyke",
        "outputId": "5820f700-f383-4b66-81b5-9bd28b2d18c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zTdawtZveDpr"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"drive/MyDrive/Colab Notebooks/data/dog-vision\"\n",
        "# \"drive/My Drive/Dog Vision\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "li0gOTcULS6x"
      },
      "outputs": [],
      "source": [
        "# Checkout the labels of our data\n",
        "import pandas as pd\n",
        "labels_csv = pd.read_csv(f\"{DATA_PATH}/labels.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zNHIfJyetdC"
      },
      "source": [
        "### Getting images and their labels \n",
        "\n",
        "Let's get a list of all of our image file pathnames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FIPj4LcdPftF"
      },
      "outputs": [],
      "source": [
        "# Create pathnames from imageID's\n",
        "filenames = [f\"{DATA_PATH}/train/{fname}.jpg\" for fname in labels_csv[\"id\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI88t6UUQAOF",
        "outputId": "35a18e44-aba2-4a3e-9f0c-b33afa405545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filenames match actual amount of files!!! Proceed.\n"
          ]
        }
      ],
      "source": [
        "# Check weather number of filenames matches number of actual image files\n",
        "import os\n",
        "if len(os.listdir(f\"{DATA_PATH}/train/\")) == len(filenames):\n",
        "  print(\"Filenames match actual amount of files!!! Proceed.\")\n",
        "else:\n",
        "  print(\"Filenames do no match actual amount of files, check the target directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgW-tzq7enNp"
      },
      "source": [
        "Since we've now got our training image filepaths in a list, let's prepare our labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azJZR9H7Ta2A",
        "outputId": "a51d7afe-d38b-4d4f-e82a-ebdedf8d3a9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['boston_bull', 'dingo', 'pekinese', ..., 'airedale',\n",
              "       'miniature_pinscher', 'chesapeake_bay_retriever'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import numpy as np\n",
        "labels = labels_csv[\"breed\"].to_numpy() \n",
        "# labels = np.array(labels) # does same thing as above\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmnY040cTfau",
        "outputId": "13007e2c-0bd7-4e14-c6bc-825274384168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels matches number of filenames!\n"
          ]
        }
      ],
      "source": [
        "# See if number of labels matches the number of filenames\n",
        "if len(labels) == len(filenames):\n",
        "  print(\"Number of labels matches number of filenames!\")\n",
        "else:\n",
        "  print(\"Number of labels does not match number of filenames, check data directories!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ReKkinxFUXzb"
      },
      "outputs": [],
      "source": [
        "# Find the unique label values\n",
        "unique_breeds = np.unique(labels)\n",
        "\n",
        "# Turn every label into a boolean array\n",
        "boolean_labels = [label == unique_breeds for label in labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzIUTkhALL6g"
      },
      "source": [
        "### Creating our own validation set\n",
        "Since the dataset from Kaggle doesn't come with a validation set, we're going to create our own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oxAIO0vYLOL3"
      },
      "outputs": [],
      "source": [
        "# Setup X & y variables\n",
        "X = filenames\n",
        "y = boolean_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUBaq7yJLl3w"
      },
      "source": [
        "We're going to start off experimenting with ~1000 images and increase as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5hyr75V4Lpzg"
      },
      "outputs": [],
      "source": [
        "# Set number of images to use for experimenting\n",
        "NUM_IMAGES = 1000 #@param {type:\"slider\", min:1000, max:10000, step:1000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VxDExTgL-8x",
        "outputId": "69ba07ec-f093-47d5-be5c-c0e8c8097bc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 800, 200, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Let's split our data into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split them into training and validation of total size NUM_IMAGES\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],\n",
        "                                                  y[:NUM_IMAGES],\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42)\n",
        "\n",
        "len(X_train), len(y_train), len(X_val), len(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlJ9_DSRfAFf"
      },
      "source": [
        "## Preprocessing Images (turning images into Tensors)\n",
        "\n",
        "To preprocess our images into Tensors we're going to write a function which does a few things:\n",
        "1. Take an image filepath as input\n",
        "2. Use TensorFlow to read the file and save it to a variable, `image`\n",
        "3. Turn our `image` (a jpg) into Tensors\n",
        "4. Normalize our image (convert color channel values from 0-255 to 0-1).\n",
        "5. Resize the `image` to be a shape of (224, 224)\n",
        "6. Return the modified `image`\n",
        "\n",
        "Before we do, let's see what importing an image looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uywBPKEXdO7D"
      },
      "outputs": [],
      "source": [
        "# Define image size\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create a function for preprocessing images\n",
        "def process_image(image_path, img_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Takes an image file path and turns the image into a Tensor.\n",
        "  \"\"\"\n",
        "  # Read in an image file\n",
        "  image = tf.io.read_file(image_path)\n",
        "  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  # Convert the colour channel values from 0-255 to 0-1 values\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize the image to our desired value (224, 224)\n",
        "  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqecJ_R0fLNb"
      },
      "source": [
        "## Turning our data into batches\n",
        "\n",
        "Why turn our data into batches?\n",
        "\n",
        "Let's say you're trying to process 10,000+ images in one go... they all might not fit into memory.\n",
        "\n",
        "So that's why we do about 32 (this is the batch size) images at a time (you can manually adjust the batch size if need be).\n",
        "\n",
        "In order to use TensorFlow effectively, we need our data in the form of Tensor tuples which look like this: \n",
        "`(image, label)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3L8Kt6HhdO7D"
      },
      "outputs": [],
      "source": [
        "# Create a simple function to return a tuple (image, label)\n",
        "def get_image_label(image_path, label):\n",
        "  \"\"\"\n",
        "  Takes an image file path name and the assosciated label,\n",
        "  processes the image and reutrns a typle of (image, label).\n",
        "  \"\"\"\n",
        "  image = process_image(image_path)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugehOjKmfUs4"
      },
      "source": [
        "Now we've got a way to turn our data into tuples of Tensors in the form: `(image, label)`, let's make a function to turn all of our data (`X` & `y`) into batches!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cdKrLRSOdO7D"
      },
      "outputs": [],
      "source": [
        "# Define the batch size, 32 is a good start\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create a function to turn data into batches\n",
        "def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n",
        "  \"\"\"\n",
        "  Creates batches of data out of image (X) and label (y) pairs.\n",
        "  Shuffles the data if it's training data but doesn't shuffle if it's validation data.\n",
        "  Also accepts test data as input (no labels).\n",
        "  \"\"\"\n",
        "  # If the data is a test dataset, we probably don't have have labels\n",
        "  if test_data:\n",
        "    print(\"Creating test data batches...\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths (no labels)\n",
        "    data_batch = data.map(process_image).batch(batch_size)\n",
        "  \n",
        "  # If the data is a valid dataset, we don't need to shuffle it\n",
        "  elif valid_data:\n",
        "    print(\"Creating validation data batches...\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths\n",
        "                                               tf.constant(y))) # labels\n",
        "    data_batch = data.map(get_image_label).batch(batch_size)\n",
        "\n",
        "  else:\n",
        "    print(\"Creating training data batches...\")\n",
        "    # Turn filepaths and labels into Tensors\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                               tf.constant(y)))\n",
        "    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images\n",
        "    data = data.shuffle(buffer_size=len(X))\n",
        "\n",
        "    # Create (image, label) tuples (this also turns the iamge path into a preprocessed image)\n",
        "    data = data.map(get_image_label)\n",
        "\n",
        "    # Turn the training data into batches\n",
        "    data_batch = data.batch(batch_size)\n",
        "  return data_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WefkDlU8dO7E",
        "outputId": "8cbc0617-3892-4262-8f32-5dd5ed37e060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training data batches...\n",
            "Creating validation data batches...\n"
          ]
        }
      ],
      "source": [
        "# Create training and validation data batches\n",
        "train_data = create_data_batches(X_train, y_train)\n",
        "val_data = create_data_batches(X_val, y_val, valid_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VQlO7KiBSWEk"
      },
      "outputs": [],
      "source": [
        "# Create a function to load a trained model\n",
        "def load_model(model_path):\n",
        "  \"\"\"\n",
        "  Loads a saved model from a specified path.\n",
        "  \"\"\"\n",
        "  print(f\"Loading saved model from: {model_path}\")\n",
        "  model = tf.keras.models.load_model(model_path, \n",
        "                                     custom_objects={\"KerasLayer\":hub.KerasLayer})\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGFSdqfSavp",
        "outputId": "4d47c93a-921c-48e9-b18a-6d9cf29413a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved model from: drive/MyDrive/Colab Notebooks/data/dog-vision/models/20230130-05511675057901-1000-images-mobilenetv2-Adam.h5\n"
          ]
        }
      ],
      "source": [
        "# Load a trained model\n",
        "loaded_1000_image_model = load_model(f\"{DATA_PATH}/models/20230130-05511675057901-1000-images-mobilenetv2-Adam.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "kPGNEGMZY-Sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making and evaluating predictions using a trained model "
      ],
      "metadata": {
        "id": "G79vM8jDZfMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = loaded_1000_image_model"
      ],
      "metadata": {
        "id": "ytZJktHeZwdd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_47_ewlZZpKj",
        "outputId": "3af02ea3-ec06-4b50-9ff2-626f6c58c533"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 120), dtype=tf.bool, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(val_data, verbose=1)\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRSSP5YHZjbP",
        "outputId": "7343d96c-254d-48ef-82b4-0436e96e4881"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 13s 1s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.25392037e-03, 3.34030556e-05, 5.87509596e-04, ...,\n",
              "        4.73118744e-05, 3.62558217e-06, 1.43094501e-03],\n",
              "       [2.90242443e-03, 1.85185601e-03, 3.75254676e-02, ...,\n",
              "        7.77256500e-05, 6.68112305e-04, 3.13210214e-04],\n",
              "       [6.89202716e-05, 3.23318673e-05, 2.52661757e-05, ...,\n",
              "        3.24379871e-05, 4.57434107e-05, 1.91236992e-04],\n",
              "       ...,\n",
              "       [7.44460579e-07, 1.24601011e-05, 5.27296324e-05, ...,\n",
              "        1.43923189e-05, 1.61705211e-05, 4.21006334e-05],\n",
              "       [7.96642900e-03, 1.06169260e-04, 2.29836616e-04, ...,\n",
              "        1.07955406e-04, 4.51440610e-05, 7.18181999e-03],\n",
              "       [5.76020975e-04, 7.70392944e-05, 5.66613744e-04, ...,\n",
              "        1.94099580e-03, 2.56530242e-03, 5.85808011e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0]  # using softmax activation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgAxb3Z2Z-CV",
        "outputId": "7cdc8145-2ab8-4dfd-fa35-f425a1e1f3ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.25392037e-03, 3.34030556e-05, 5.87509596e-04, 1.82466698e-04,\n",
              "       6.18016697e-04, 6.77973730e-05, 3.94250713e-02, 2.50931684e-04,\n",
              "       3.89839028e-04, 8.64737725e-04, 1.36085815e-04, 2.70503078e-05,\n",
              "       2.94261728e-04, 1.57569521e-05, 8.80287902e-04, 8.18714558e-04,\n",
              "       4.09523564e-05, 3.53939623e-01, 7.61584488e-06, 9.06319401e-05,\n",
              "       1.26225292e-04, 1.64255121e-04, 1.12546941e-05, 2.31786445e-03,\n",
              "       2.43164704e-05, 1.80907271e-04, 2.04054102e-01, 9.09653245e-05,\n",
              "       9.69130197e-05, 2.64032104e-04, 5.34940162e-04, 1.17940817e-03,\n",
              "       2.89182906e-04, 4.68568760e-05, 1.32750196e-04, 1.40508693e-02,\n",
              "       7.99532427e-06, 1.09747157e-03, 7.62372511e-05, 1.36382863e-04,\n",
              "       6.33108080e-04, 4.92421213e-06, 7.05859784e-05, 2.58097745e-04,\n",
              "       1.80031748e-05, 1.86109814e-04, 7.97800749e-05, 2.48167253e-05,\n",
              "       1.57213799e-04, 2.18986344e-04, 1.24590879e-04, 2.67514351e-05,\n",
              "       4.96608147e-04, 1.79257713e-05, 1.24567423e-05, 1.97273021e-05,\n",
              "       1.35827722e-04, 8.92725366e-04, 6.67068642e-04, 1.36059403e-01,\n",
              "       2.61446636e-04, 2.51800648e-05, 1.05303840e-03, 1.52308257e-05,\n",
              "       1.40725228e-04, 3.35722230e-03, 7.20495882e-05, 6.19780913e-05,\n",
              "       1.24777341e-03, 1.20750941e-04, 1.13190636e-02, 8.83028551e-05,\n",
              "       1.42197852e-04, 8.07640050e-03, 9.50477901e-04, 1.52232442e-05,\n",
              "       5.21373050e-03, 9.89553425e-03, 2.21541486e-04, 6.42333319e-03,\n",
              "       6.59356709e-04, 1.08338799e-02, 4.84011507e-05, 4.60564904e-03,\n",
              "       5.93397499e-06, 6.98833552e-04, 1.65408768e-04, 2.93191813e-04,\n",
              "       6.83059261e-05, 8.54881248e-04, 5.64250979e-04, 9.53237468e-05,\n",
              "       8.13928273e-06, 1.96427130e-03, 8.08244076e-05, 2.65577139e-04,\n",
              "       2.00540017e-04, 1.32642034e-02, 1.70017104e-03, 1.13261085e-04,\n",
              "       4.80035739e-03, 1.86330726e-04, 2.46551987e-02, 4.06071208e-02,\n",
              "       1.04049956e-04, 7.51878833e-04, 4.09838893e-02, 3.49635957e-05,\n",
              "       2.23314724e-04, 3.35149206e-02, 2.79763161e-04, 6.76013355e-04,\n",
              "       8.56869610e-06, 7.56778900e-05, 2.56792235e-04, 3.77693796e-05,\n",
              "       1.21770217e-03, 4.73118744e-05, 3.62558217e-06, 1.43094501e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtI8XE-nas_b",
        "outputId": "a44ef405-68a2-4b66-e2a7-24450dbcd12b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmHJgTTzau5j",
        "outputId": "6525f6f6-bb70-4df0-d0d4-fc38d89d0fc7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 120)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxBAJGFuaxPh",
        "outputId": "e71f0407-db85-42f0-fce0-5503c1fbe0f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000002"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(predictions[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buUZBBexa769",
        "outputId": "0cf613ef-f9e7-4d61-efd3-56d78a8e8c8e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000001"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First prediction\n",
        "index = 42\n",
        "print(predictions[index])\n",
        "print(f\"Max value (probability of prediction): {np.max(predictions[index])}\")\n",
        "print(f\"Sum: {np.sum(predictions[index])}\")\n",
        "print(f\"Max index: {np.argmax(predictions[index])}\")\n",
        "print(f\"Predicted label: {unique_breeds[np.argmax(predictions[index])]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhuZsFcrbYrt",
        "outputId": "c5b1fa6a-3805-4100-97f1-4cc8765d8cd5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.50392584e-05 5.86264105e-05 1.45507147e-05 7.16192471e-06\n",
            " 5.62532572e-04 8.74987109e-06 4.55970294e-05 4.61054151e-04\n",
            " 3.45369126e-03 1.49791269e-02 4.32126762e-06 1.35657262e-06\n",
            " 3.27824615e-04 2.08758214e-03 3.85656342e-04 4.11873334e-04\n",
            " 2.30520272e-05 5.01037612e-05 5.74141704e-05 1.00697966e-04\n",
            " 1.34415950e-06 2.44295341e-04 8.91110267e-06 1.60881355e-05\n",
            " 2.86730542e-03 1.93116157e-05 1.02197282e-05 3.05112590e-05\n",
            " 8.63541791e-05 6.24165614e-06 1.20076775e-05 7.12570400e-05\n",
            " 1.74812176e-05 7.23716175e-06 1.90639075e-05 2.43896939e-05\n",
            " 8.21923750e-05 9.81947087e-05 1.31685247e-05 3.91808003e-02\n",
            " 1.79603339e-05 6.53347524e-06 2.78224726e-03 1.07864196e-06\n",
            " 4.48248284e-05 2.81346511e-05 1.71700813e-05 2.69971468e-04\n",
            " 3.08141753e-05 1.35936221e-04 1.44374517e-05 1.17084974e-05\n",
            " 1.93901680e-04 2.24136957e-03 3.78847585e-06 2.59254943e-04\n",
            " 8.34026796e-05 3.61426974e-05 4.45251826e-05 6.78372999e-06\n",
            " 1.14997338e-05 4.33694222e-04 1.15528201e-06 6.63887186e-05\n",
            " 1.14162167e-05 3.60648846e-05 1.63100067e-05 2.83029163e-04\n",
            " 1.23657243e-04 1.66390837e-05 9.06810692e-06 5.29547506e-05\n",
            " 3.28069327e-05 4.35082402e-05 4.88400792e-06 6.04449742e-05\n",
            " 1.63998688e-04 1.64466292e-05 4.47324601e-05 1.50848573e-04\n",
            " 2.39645419e-06 3.51765666e-05 5.98937622e-05 1.60595388e-04\n",
            " 1.03704289e-04 9.06269543e-06 5.64057918e-05 1.12483337e-06\n",
            " 7.80436312e-06 1.88301958e-04 3.75565542e-05 1.74963691e-06\n",
            " 1.92510386e-04 2.43230534e-05 1.88555405e-05 7.37096934e-06\n",
            " 3.31864940e-05 3.36561716e-05 2.06264922e-05 5.09635429e-05\n",
            " 1.27867515e-05 6.76110867e-05 8.11749414e-05 1.37435336e-05\n",
            " 2.33418923e-05 8.59194188e-06 2.42774204e-05 3.07513687e-06\n",
            " 1.97631143e-05 3.30305193e-05 2.02335286e-06 2.93446181e-04\n",
            " 1.04914121e-04 9.23547328e-01 9.69907487e-05 1.35722745e-04\n",
            " 9.78071694e-06 1.83096090e-05 6.13786629e-04 1.43077530e-04]\n",
            "Max value (probability of prediction): 0.9235473275184631\n",
            "Sum: 1.0\n",
            "Max index: 113\n",
            "Predicted label: walker_hound\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second prediction\n",
        "index = 0\n",
        "print(f\"Max value (probability of prediction): {np.max(predictions[index])}\")\n",
        "print(f\"Sum: {np.sum(predictions[index])}\")\n",
        "print(f\"Max index: {np.argmax(predictions[index])}\")\n",
        "print(f\"Predicted label: {unique_breeds[np.argmax(predictions[index])]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gttQtsCvcVWg",
        "outputId": "a53d555e-24fa-4b40-f8b6-b627391da77e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max value (probability of prediction): 0.35393962264060974\n",
            "Sum: 1.000000238418579\n",
            "Max index: 17\n",
            "Predicted label: border_terrier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having the above functionality is great but we want to be able to do it at scale.\n",
        "\n",
        "And it would be even better if we could see the image the prediction is being made on!\n",
        "\n",
        "**Note:** Prediction probabilities are also known as confidence levels."
      ],
      "metadata": {
        "id": "JCiZ9i79a3_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilities into their respective label (easier to understand)\n",
        "def get_pred_label(prediction_probabilities):\n",
        "  \"\"\"\n",
        "  Turns an array of prediction probabilities into a label.\n",
        "  \"\"\"\n",
        "  return unique_breeds[np.argmax(prediction_probabilities)]\n",
        "\n",
        "# Get a predicted label based on an array of prediction probabilities\n",
        "pred_label = get_pred_label(predictions[81])\n",
        "pred_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LjtfsWFybGXQ",
        "outputId": "fb49c5b1-5c5c-4ac0-bf08-8ff9c3e94caa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'brittany_spaniel'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since our validation data is still in a batch dataset, we'll have to unbatchify it to make predictions on the validation images and then compare those predictions to the validation labels (truth labels)."
      ],
      "metadata": {
        "id": "y7zPJut9bv0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0N50iI_cCOu",
        "outputId": "59bb1d12-d711-47aa-a6b3-f7551a309954"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 120), dtype=tf.bool, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to unbatch a batch dataset\n",
        "def unbatchify(data):\n",
        "  \"\"\"\n",
        "  Takes a batched dataset of (image, label) Tensors and reutrns separate arrays\n",
        "  of images and labels.\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  labels = []\n",
        "  # Loop through unbatched data\n",
        "  for image, label in data.unbatch().as_numpy_iterator():\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "  return images, labels\n",
        "\n",
        "# Unbatchify the validation data\n",
        "val_images, val_labels = unbatchify(val_data)\n",
        "val_images[0], val_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTExW2iZcHed",
        "outputId": "02596c8b-f3d6-48da-84d3-a7c67305f560"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[0.29599646, 0.43284872, 0.3056691 ],\n",
              "         [0.26635826, 0.32996926, 0.22846507],\n",
              "         [0.31428418, 0.27701408, 0.22934894],\n",
              "         ...,\n",
              "         [0.77614343, 0.82320225, 0.8101595 ],\n",
              "         [0.81291157, 0.8285351 , 0.8406944 ],\n",
              "         [0.8209297 , 0.8263737 , 0.8423668 ]],\n",
              " \n",
              "        [[0.2344871 , 0.31603682, 0.19543913],\n",
              "         [0.3414841 , 0.36560842, 0.27241898],\n",
              "         [0.45016077, 0.40117094, 0.33964607],\n",
              "         ...,\n",
              "         [0.7663987 , 0.8134138 , 0.81350833],\n",
              "         [0.7304248 , 0.75012016, 0.76590735],\n",
              "         [0.74518913, 0.76002574, 0.7830809 ]],\n",
              " \n",
              "        [[0.30157745, 0.3082587 , 0.21018331],\n",
              "         [0.2905954 , 0.27066195, 0.18401104],\n",
              "         [0.4138316 , 0.36170745, 0.2964005 ],\n",
              "         ...,\n",
              "         [0.79871625, 0.8418535 , 0.8606443 ],\n",
              "         [0.7957738 , 0.82859945, 0.8605655 ],\n",
              "         [0.75181633, 0.77904975, 0.8155256 ]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0.9746779 , 0.9878955 , 0.9342279 ],\n",
              "         [0.99153054, 0.99772066, 0.9427856 ],\n",
              "         [0.98925114, 0.9792082 , 0.9137934 ],\n",
              "         ...,\n",
              "         [0.0987601 , 0.0987601 , 0.0987601 ],\n",
              "         [0.05703771, 0.05703771, 0.05703771],\n",
              "         [0.03600177, 0.03600177, 0.03600177]],\n",
              " \n",
              "        [[0.98197854, 0.9820659 , 0.9379411 ],\n",
              "         [0.9811992 , 0.97015417, 0.9125648 ],\n",
              "         [0.9722316 , 0.93666023, 0.8697186 ],\n",
              "         ...,\n",
              "         [0.09682598, 0.09682598, 0.09682598],\n",
              "         [0.07196062, 0.07196062, 0.07196062],\n",
              "         [0.0361607 , 0.0361607 , 0.0361607 ]],\n",
              " \n",
              "        [[0.97279435, 0.9545954 , 0.92389745],\n",
              "         [0.963602  , 0.93199134, 0.88407487],\n",
              "         [0.9627158 , 0.91253304, 0.8460338 ],\n",
              "         ...,\n",
              "         [0.08394483, 0.08394483, 0.08394483],\n",
              "         [0.0886985 , 0.0886985 , 0.0886985 ],\n",
              "         [0.04514172, 0.04514172, 0.04514172]]], dtype=float32),\n",
              " array([False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False,  True,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pred_label(val_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wC65Gm21haWE",
        "outputId": "18635578-04d7-40a0-87a7-df99a4bd30e6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cairn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "434d186b2765b94255b656aa41f17eff7bec9ec240788e87583f679a1c7ef224"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}